---
title: "Modeling"
pagetitle: 'Modeling'
output: 
  html_document: 
    css: "styles.css"
    toc: true
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE, echo=TRUE}
library(reticulate)
use_python(Sys.which("python3")) # Use python3
Sys.which("python3")
```


```{r html-setup, include=FALSE}
htmltools::tagList(rmarkdown::html_dependency_font_awesome())

```
## Modeling Summary

In order to complete the model training step, we will follow the following steps from class:

1. Define your Model Class

2. Define the Cost Function

3. Perform Optimization

4. Check the Performance of Fitted Model

## Defining Our Model class

We know that our output will be a number so we will use some type of regression model. Let's try out a few regression models

## Import model libraries
```{python importing}
import sklearn
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
matplotlib.use('Agg')
```
## Data Preparation

### Pre-processing 1: Normalizing data
```{python normalization}
df = pd.read_csv('clean_train.csv')

# create a scaler object
std_scaler = StandardScaler()
# fit and transform the data
numeric_cols = list(df.select_dtypes(include=['int64','float64']).columns)

# Remove SalePrice from the values to be normalized
numeric_cols.remove("SalePrice")
# Normalize all numeric values
df[numeric_cols] = pd.DataFrame(std_scaler.fit_transform(df[numeric_cols]),
  columns=numeric_cols)
df
```


### Pre-processing 2: Creating Dummy Variables for categorircal variables
```{python dummies}


# Create dummy variable columns for the categorical variables
categorical_columns = df.select_dtypes(include=['object','bool']).columns
for column in categorical_columns:
    # Print out the column names
    print(f"column {column}: ",df[column].unique())
    dummies = pd.get_dummies(df[column]).rename(columns= lambda x: column +'_' + str(x))
    df = pd.concat([df, dummies], axis=1)
    df = df.drop([column], axis=1)




```
### Pre-processing 3: Dropping NA columns
```{python droppingNAs}

print(df.shape)
df.dropna(inplace=True)
print(df.shape)
```


### Pre-processing 4: Creating test and training datasets
```{python training-separation}

# Create dataframes for independent and dependent variables
# Dependent variable
y = df['SalePrice']
# Independent Variable
X = df.drop('SalePrice', axis=1)

# Create training and test sets. Test is .25 of data
X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=.25, random_state=123)
```























## Random Forest Regressor
```{python RFRegressor}
from sklearn.ensemble import RandomForestRegressor
rf_mses = []
max_depths = []
for d in range(1,30):
    rf_regr = RandomForestRegressor(max_depth=d, random_state=0)
    # Fit the data to the model
    rf_regr.fit(X_train, y_train)
    # Make predictions on test data
    rf_y_pred = rf_regr.predict(X_test)
    # Report Mean Square Error
    rf_mses.append(mean_squared_error(y_test, rf_y_pred))
    max_depths.append(d)
    #print(f"Mean Squared Error: {rf_mses[-1]}")
best_n = max_depths[rf_mses.index(min(rf_mses))]
rf_mse = min(rf_mses)
print(f"Best MSE of {rf_mse} at max_depth of {best_n}")

import matplotlib.pyplot as plt
plt.close()
plt.plot(max_depths, rf_mses)
plt.show()

# Now that we know the best n, train again
rf_regr = RandomForestRegressor(max_depth=best_n, random_state=0)
# Fit the data to the model
rf_regr.fit(X_train, y_train)
# Make predictions on test data
rf_y_pred = rf_regr.predict(X_test)

```
### Defining your Model Class

Our model class is scikit-learn's random forest regressor. The model class works by building decision trees on different sub-samples of the data and uses averaging to improve the predictive accuracy. More information abou the model can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank"> Here</a>

### Defining the Cost Function

The cost function used for our random forest regressor is mean squared error. We chose this because it only slightly penalizes the model for being slightly off, but greatly penalizes the model when it is off a greater amount.

### Perform Optimization

We optimized the performance of our model by trying different max-depths for the random forest model. This helped us avoid over fitting. We went with a model that would minimize the mean square error of the model

### Check RF Performance

The performance of the model is checked below:

```{python RFRegressor-Eval}
rf_mse = mean_squared_error(y_test, rf_y_pred)
print(f"Mean Squared Error: {rf_mse}")

max_val = max(max(y_test),max(rf_y_pred))
plt.close()
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(rf_y_pred, y_test, '.y')
plt.xlabel('Random Forest Regressor Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title(f'Random Forest Regressor Diagnosis Plot (max_depth={best_n})')
plt.show()
# Report Mean Square Error
print(f"Random Forest RMSE: {rf_mse:,}")
print(f"Random Forest R^2: {rf_regr.score(X_test, y_test)}")
```



























## Linear Regression
```{python LinearRegressor}
# Train a linear model
lin_regr = LinearRegression(fit_intercept=False)
lin_regr.fit(X_train, y_train)
# Make predictions on test data
lin_y_pred = lin_regr.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_y_pred)
```
```{python testing}
import statsmodels.api as sm
from scipy import stats

X2 = sm.add_constant(X_train)
est = sm.OLS(y_train, X2)
lin_regr = est.fit()
#print(lin_regr.summary())
lin_y_pred = lin_regr.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_y_pred)
print(lin_mse)
print(lin_regr.rsquared)
```


```{python Linear-Eval}
max_val = max(max(y_test),max(lin_y_pred))
plt.close()
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(lin_y_pred, y_test, '.r')
plt.xlabel('Linear Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title('Linear Model Diagnosis Plot')
plt.legend(['Perfect fit line',"Linear Model"])
plt.show()
print(f"Linear MSE: {lin_mse:,}")
print(f"Linear Reg R^2: {lin_regr.rsquared}")
```




















## KNN Regressor
```{python KNNRegressor}
from sklearn.neighbors import KNeighborsRegressor
knn_mses = []
nums_neighbors = []
for k in range(1,30):
    knn_regr = KNeighborsRegressor(
      n_neighbors=k,
      weights="distance"
    )
    # Fit the data to the model
    knn_regr.fit(X_train, y_train)
    # Make predictions on test data
    knn_y_pred = knn_regr.predict(X_test)
    # Report Mean Square Error
    knn_mses.append(mean_squared_error(y_test, knn_y_pred))
    nums_neighbors.append(k)
    #print(f"Mean Squared Error: {knn_mses[-1]}")
best_k = nums_neighbors[knn_mses.index(min(knn_mses))]
knn_mse = min(knn_mses)
print(f"Best MSE of {knn_mse} at n_neighbors of {best_k}")

plt.close()
plt.plot(nums_neighbors, knn_mses, c="red")
plt.title('MSE of different n_neighbors for KNN')
plt.xlabel('Number of Neighbors', size=20)
plt.ylabel('Mean Square Error', size=20)

plt.rcParams["figure.autolayout"] = True
plt.show()



# Now that we know the best n, train again
knn_regr = KNeighborsRegressor(
  n_neighbors=best_k,
  weights="distance"
  )
# Fit the data to the model
knn_regr.fit(X_train, y_train)
# Make predictions on test data
knn_y_pred = knn_regr.predict(X_test)


```
### Defining your Model Class

Our model class is scikit-learn's K-nearest neighbors regressor. The model class works by basically memorizing a list of points for comparing future points to - these future points are compared to the k nearest neighbors and take the weighted average of their values (weights are determined by distance). More information abou the model can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank"> Here</a>

### Defining the Cost Function

KNN does not exactly have a cost function - instead, the KNN model basically just memorizes the locations of the training dataset and compares future prediction points to the points the model was trained on. The model then chooses the weighted average of the k-closest points

### Perform Optimization

We optimized the performance of our model by trying different numbers of neighbors (k) for the KNN model. This helped us determine which K values would be best for our model.

### Check KNN Performance

The performance of the model is checked below:

```{python KNN-eval}
plt.close()
max_val = max(max(y_test),max(knn_y_pred))
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(knn_y_pred, y_test, '.b')
plt.xlabel('KNN Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title(f'KNN Regressor Diagnosis Plot (neighbors={best_k})')
plt.legend(['Perfect fit line',"KNN Model"])
plt.show()

# Report Mean Square Error
print(f"MSE of KNN: {mean_squared_error(y_test, knn_y_pred)}")
print(f"KNN R^2 score: {knn_regr.score(X_test, y_test)}")
```




















## Neural Network Regressor
```{python neural-network}
from sklearn.neural_network import MLPRegressor
vals = []
combos = []
mlp_regr = MLPRegressor(random_state=1,
  max_iter=500, 
  hidden_layer_sizes=(100,30,8),
  learning_rate="constant", # {‘constant’, ‘invscaling’, ‘adaptive’}
  alpha=10**-6, # default =0.0001
  solver="adam",
  batch_size=200
  ).fit(X_train, y_train)
MLP_y_pred = mlp_regr.predict(X_test)
print(mlp_regr.score(X_test, y_test))
```
### Defining your Model Class

Our model class is scikit-learn's MLP. The model works by creating and training a neural network in epochs. We trained parameters More information about the model can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html" target="_blank"> Here</a>

### Defining the Cost Function

The cost function in the neural network minimizes 

### Perform Optimization

We optimized the performance of our model by trying different numbers of neighbors (k) for the KNN model. This helped us determine which K values would be best for our model.

### Check Neural Net Performance

The performance of the model is checked below:

```{python NeuralNetwork-eval}
plt.close()
max_val = max(max(y_test),max(MLP_y_pred))
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(MLP_y_pred, y_test, '.g')
plt.xlabel('Neural Network Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price (%)', size=20)
plt.title('Neural Network Model Diagnosis Plot')
plt.legend(['Perfect fit line',"Neural Network Model"])
plt.show()

# Report Mean Square Error
print(f"MSE of Neural Network: {mean_squared_error(y_test, MLP_y_pred)}")
print(f"Neural Network R^2 score: {mlp_regr.score(X_test, y_test)}")
```


## Comparing Models (Evaluation)
```{python evaluation}

lin_mse = mean_squared_error(y_test, lin_y_pred)
knn_mse = mean_squared_error(y_test, knn_y_pred)
rf_mse = mean_squared_error(y_test, rf_y_pred)
MLP_mse = mean_squared_error(y_test, MLP_y_pred)

max_val = max(
  max(y_test),
  max(MLP_y_pred),
  max(rf_y_pred), 
  max(knn_y_pred)
)
plt.close()
plt.xlim([0, max_val])
plt.xlim([0, max_val])

plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(lin_y_pred, y_test, '.r',alpha=0.3)
plt.plot(knn_y_pred, y_test, '.b',alpha=0.3)
plt.plot(rf_y_pred, y_test, '.y',alpha=0.3)
plt.plot(MLP_y_pred, y_test, '.g',alpha=0.3)
# apply legend()
plt.xlabel('Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title('Model Diagnosis Plot (all models)')

plt.legend(['Perfect fit line',"Linear Model",f"KNN Model k={best_k}",f"RF Model md={best_n}", "Neural Network Model"])
plt.show()
print(f"""
MSEs (Mean Square Errors)---------------
Linear Reg:        {lin_mse:,}
KNN Reg:           {knn_mse:,}
Random Forest Reg: {rf_mse:,}
Neural Network Reg: {MLP_mse:,}
R^2 Scores---------------
Linear Reg: {lin_regr.rsquared}
KNN Reg: {knn_regr.score(X_test, y_test)}
Random Forest Reg: {rf_regr.score(X_test, y_test)}
Neural Network Reg: {mlp_regr.score(X_test, y_test)}""")
```