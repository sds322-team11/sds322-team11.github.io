---
title: "Modeling"
pagetitle: 'Modeling'
output: 
  html_document: 
    css: "styles.css"
    toc: true
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE, echo=TRUE}
library(reticulate)
use_python(Sys.which("python3")) # Use python3
Sys.which("python3")
```


```{r html-setup, include=FALSE}
htmltools::tagList(rmarkdown::html_dependency_font_awesome())

```
## Modeling Summary

In order to complete the model training step, we will follow the following steps from class:

1. Define your Model Class

2. Define the Cost Function

3. Perform Optimization

4. Check the Performance of Fitted Model

## Defining Our Model class

We know that our output will be a number so we will use some type of regression model. Let's try out a few regression models

## Import model libraries
```{python importing}
import sklearn
import pandas as pd
import numpy as np
# Import Models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
import statsmodels.api as sm
from scipy import stats
# Import preprocessing functions/classes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.metrics import mean_squared_error, mean_absolute_error
# Import visualization tools
import matplotlib.pyplot as plt
import matplotlib


matplotlib.use('Agg')
```
## Data Preparation

### Pre-processing 1: Normalizing data
```{python normalization}
df = pd.read_csv('clean_train.csv')

# create a scaler object
std_scaler = StandardScaler()
# fit and transform the data
numeric_cols = list(df.select_dtypes(include=['int64','float64']).columns)

# Remove SalePrice from the values to be normalized
numeric_cols.remove("SalePrice")
print(df[numeric_cols])
# Normalize all numeric values
df[numeric_cols] = pd.DataFrame(std_scaler.fit_transform(df[numeric_cols]),
  columns=numeric_cols)
print(df[numeric_cols])
```


### Pre-processing 2: Creating Dummy Variables for categorircal variables
```{python dummies}


# Create dummy variable columns for the categorical variables
categorical_columns = df.select_dtypes(include=['object','bool']).columns
for column in categorical_columns:
    # Print out the column names
    print(f"""---------------------------------------------------------
column {column}:
{df[column].unique()}
""")
    dummies = pd.get_dummies(df[column]).rename(columns= lambda x: column +'_' + str(x))
    df = pd.concat([df, dummies], axis=1)
    df = df.drop([column], axis=1)


```
### Pre-processing 3: Dropping NA rows and Id column
```{python droppingNAs}
rows1 = df.shape[0]
df.drop(["Id"], axis=1, inplace=True)
df.dropna(inplace=True)

rows2 = df.shape[0]
print(f"""
Rows before: {rows1}
Rows after: {rows2}
-----------------------------
Rows dropped: {rows1 - rows2}
""")
```


### Pre-processing 4: Creating test and training datasets
```{python training-separation}

# Create dataframes for independent and dependent variables
# Dependent variable
y = df['SalePrice']
# Independent Variable
X = df.drop('SalePrice', axis=1)

# Create training and test sets. Test is .25 of data
(X_train, X_test, y_train, y_test) = train_test_split(X, y,  test_size=.25, random_state=123)
```























## Random Forest Regressor
```{python RFRegressor}

rf_mses = []
rf_training_mses = []
max_depths = []
for d in range(1,30):
    rf_regr = RandomForestRegressor(max_depth=d, random_state=0)
    # Fit the data to the model
    rf_regr.fit(X_train, y_train)
    # Make predictions on test data
    rf_y_pred = rf_regr.predict(X_test)
    rf_y_training_pred = rf_regr.predict(X_train)
    # Report Mean Square Error
    rf_mses.append(mean_squared_error(y_test, rf_y_pred))
    rf_training_mses.append(mean_squared_error(y_train, rf_y_training_pred))
    max_depths.append(d)
    #print(f"Mean Squared Error: {rf_mses[-1]}")
best_n = 9
rf_mse = min(rf_mses)
print(f"Best MSE of {rf_mse} at max_depth of {best_n}")


plt.close()
plt.plot(max_depths, rf_mses)
plt.plot(max_depths, rf_training_mses)
plt.title('MSE of different max_depth for Random Forest Regressor')
plt.xlabel('Max Depth', size=20)
plt.ylabel('Mean Square Error', size=20)
plt.legend(['Testing Data MSE',"Training Data MSE"])
plt.show()

# Now that we know the best n, train again
rf_regr = RandomForestRegressor(max_depth=best_n, random_state=0)
# Fit the data to the model
rf_regr.fit(X_train, y_train)
# Make predictions on test data
rf_y_pred = rf_regr.predict(X_test)

```
### Defining your Model Class

Our model class is scikit-learn's random forest regressor. The model class works by building decision trees on different sub-samples of the data and uses averaging to improve the predictive accuracy. More information abou the model can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank"> Here</a>

### Defining the Cost Function

The cost function used for our random forest regressor is mean squared error. We chose this because it only slightly penalizes the model for being slightly off, but greatly penalizes the model when it is off a greater amount.

### Perform Optimization

We optimized the performance of our model by trying different max-depths for the random forest model. This helped us avoid over fitting. We went with a model that would minimize the mean square error of the model

### Check RF Performance

The performance of the model is checked below:

```{python RFRegressor-Eval}
rf_mse = mean_squared_error(y_test, rf_y_pred)


max_val = max(max(y_test),max(rf_y_pred))
plt.close()
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(rf_y_pred, y_test, '.y')
plt.xlabel('Random Forest Regressor Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title(f'Random Forest Regressor Diagnosis Plot (max_depth={best_n})')

plt.show()
# Report Mean Square Error
print(f"""Random Forest (max_depth={best_n}) MSE: {mean_squared_error(y_test, rf_y_pred):,.2f}
Random Forest (max_depth={best_n}) MAE: {mean_absolute_error(y_test, rf_y_pred):,.2f}
Random Forest (max_depth={best_n}) R^2: {rf_regr.score(X_test, y_test):.3f}""")
```



























## Linear Regression
```{python LinearRegressor}
# Train a linear model
lin_regr = LinearRegression(fit_intercept=False)
lin_regr.fit(X_train, y_train)
# Make predictions on test data
lin_y_pred = lin_regr.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_y_pred)
```
```{python testing}



X2 = sm.add_constant(X_train)
est = sm.OLS(y_train, X2)
lin_regr = est.fit()
#print(lin_regr.summary())
lin_y_pred = lin_regr.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_y_pred)

#print(lin_regr.rsquared)
results_as_html = lin_regr.summary().tables[1].as_html()
anova_results = pd.read_html(results_as_html, header=0, index_col=0)[0]
anova_results.columns
anova_results.sort_values(by=["t"],ascending=False).dropna(axis=0).head(10)
```


```{python Linear-Eval}
max_val = max(max(y_test),max(lin_y_pred))
plt.close()
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(lin_y_pred, y_test, '.r')
plt.xlabel('Linear Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title('Linear Model Diagnosis Plot')
plt.legend(['Perfect fit line',"Linear Model"])
plt.show()


print(f"""Multiple Linear Regression MSE: {mean_squared_error(y_test, lin_y_pred):,.2f}
Multiple Linear Regression MAE: {mean_absolute_error(y_test, lin_y_pred):,.2f}
Multiple Linear Regression R^2: {lin_regr.rsquared:.3f}""")
```




















## KNN Regressor
```{python KNNRegressor}

knn_mses = []
nums_neighbors = []
for k in range(1,30):
    knn_regr = KNeighborsRegressor(
      n_neighbors=k,
      weights="distance"
    )
    # Fit the data to the model
    knn_regr.fit(X_train, y_train)
    # Make predictions on test data
    knn_y_pred = knn_regr.predict(X_test)
    # Report Mean Square Error
    knn_mses.append(mean_squared_error(y_test, knn_y_pred))
    nums_neighbors.append(k)
    #print(f"Mean Squared Error: {knn_mses[-1]}")
best_k = nums_neighbors[knn_mses.index(min(knn_mses))]
knn_mse = min(knn_mses)
print(f"Best MSE of {knn_mse} at n_neighbors of {best_k}")

plt.close()
plt.plot(nums_neighbors, knn_mses, c="red")
plt.title('MSE of different n_neighbors for KNN')
plt.xlabel('Number of Neighbors', size=20)
plt.ylabel('Mean Square Error', size=20)

plt.rcParams["figure.autolayout"] = True
plt.show()



# Now that we know the best n, train again
knn_regr = KNeighborsRegressor(
  n_neighbors=best_k,
  weights="distance"
  )
# Fit the data to the model
knn_regr.fit(X_train, y_train)
# Make predictions on test data
knn_y_pred = knn_regr.predict(X_test)


```
### Defining your Model Class

Our model class is scikit-learn's K-nearest neighbors regressor. The model class works by basically memorizing a list of points for comparing future points to - these future points are compared to the k nearest neighbors and take the weighted average of their values (weights are determined by distance). More information abou the model can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank"> Here</a>

### Defining the Cost Function

KNN does not exactly have a cost function - instead, the KNN model basically just memorizes the locations of the training dataset and compares future prediction points to the points the model was trained on. The model then chooses the weighted average of the k-closest points

### Perform Optimization

We optimized the performance of our model by trying different numbers of neighbors (k) for the KNN model. This helped us determine which K values would be best for our model.

### Check KNN Performance

The performance of the model is checked below:

```{python KNN-eval}
plt.close()
max_val = max(max(y_test),max(knn_y_pred))
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(knn_y_pred, y_test, '.b')
plt.xlabel('KNN Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title(f'KNN Regressor Diagnosis Plot (neighbors={best_k})')
plt.legend(['Perfect fit line',"KNN Model"])
plt.show()

# Report Mean Square Error
print(f"""KNN (k={best_k}) MSE: {mean_squared_error(y_test, knn_y_pred):,.2f}
KNN (k={best_k}) MAE: {mean_absolute_error(y_test, knn_y_pred):,.2f}
KNN (k={best_k}) R^2: {knn_regr.score(X_test, y_test):.3f}""")
```




















## Neural Network Regressor
```{python neural-network}

vals = []
combos = []
mlp_regr = MLPRegressor(random_state=1,
  max_iter=500, 
  hidden_layer_sizes=(100,30,8),
  learning_rate="constant", # {‘constant’, ‘invscaling’, ‘adaptive’}
  alpha=10**-6, # default =0.0001
  solver="adam",
  batch_size=200
  ).fit(X_train, y_train)
MLP_y_pred = mlp_regr.predict(X_test)
print(mlp_regr.score(X_test, y_test))
MLP_mae = mean_absolute_error(y_test, MLP_y_pred)
```
### Defining your Model Class

Our model class is scikit-learn's MLP. The model works by creating and training a neural network in epochs. We trained parameters More information about the model can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html" target="_blank"> Here</a>

### Defining the Cost Function

The cost function in the neural network minimizes the squared error using the "adam" stochastic gradient-based optimizer

### Perform Optimization

We tried to optimize the performance of the neural network by trying different hyperparameters for the model and keeping the new hyperparameters if the squared error improved.

### Check Neural Net Performance

The performance of the model is checked below:

```{python NeuralNetwork-eval}
plt.close()
max_val = max(max(y_test),max(MLP_y_pred))
plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(MLP_y_pred, y_test, '.g')
plt.xlabel('Neural Network Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title('Neural Network Model Diagnosis Plot')
plt.legend(['Perfect fit line',"Neural Network Model"])
plt.show()

# Report Mean Square Error
print(f"""Neural Network MSE: {mean_squared_error(y_test, MLP_y_pred):,.2f}
Neural Network MAE: {mean_absolute_error(y_test, MLP_y_pred):,.2f}
Neural Network R^2: {mlp_regr.score(X_test, y_test):.3f}""")
```


## Discussion

### Measuring Performance
```{python performance-eval}

lin_mse = mean_squared_error(y_test, lin_y_pred)
knn_mse = mean_squared_error(y_test, knn_y_pred)
rf_mse = mean_squared_error(y_test, rf_y_pred)
MLP_mse = mean_squared_error(y_test, MLP_y_pred)

lin_mae = mean_absolute_error(y_test, lin_y_pred)
knn_mae = mean_absolute_error(y_test, knn_y_pred)
rf_mae = mean_absolute_error(y_test, rf_y_pred)
MLP_mae = mean_absolute_error(y_test, MLP_y_pred)

max_val = max(
  max(y_test),
  max(MLP_y_pred),
  max(rf_y_pred), 
  max(knn_y_pred)
)
plt.close()
plt.xlim([0, max_val])
plt.xlim([0, max_val])

plt.plot([0.0, max_val], [0.0, max_val], 'k')
plt.plot(lin_y_pred, y_test, '.r',alpha=0.3)
plt.plot(knn_y_pred, y_test, '.b',alpha=0.3)
plt.plot(rf_y_pred, y_test, '.y',alpha=0.3)
plt.plot(MLP_y_pred, y_test, '.g',alpha=0.3)
# apply legend()
plt.xlabel('Model Predicted Sales Price ($)', size=20)
plt.ylabel('True Sales Price ($)', size=20)
plt.title('Model Diagnosis Plot (all models)')

plt.legend(['Perfect fit line',"Linear Model",f"KNN Model k={best_k}",f"RF Model md={best_n}", "Neural Network Model"])
plt.show()
print(f"""
MSEs (Mean Square Errors)-----------------
Linear Reg:          {lin_mse:,.2f}
KNN Reg:             {knn_mse:,.2f}
Random Forest Reg:   {rf_mse:,.2f}
Neural Network Reg:  {MLP_mse:,.2f}
MAEs (Mean Absolute Errors)---------------
Linear Reg:         ${lin_mae:,.2f}
KNN Reg:            ${knn_mae:,.2f}
Random Forest Reg:  ${rf_mae:,.2f}
Neural Network Reg: ${MLP_mae:,.2f}
R^2 Scores--------------------------------
Linear Reg:          {lin_regr.rsquared:.3f}
KNN Reg:             {knn_regr.score(X_test, y_test):.3f}
Random Forest Reg:   {rf_regr.score(X_test, y_test):.3f}
Neural Network Reg:  {mlp_regr.score(X_test, y_test):.3f}""")
```


### TODO: Findings and Interpretations
Based on the R^2 and MSE values of the models when test data is applied to them,


```{python SHAP-interpretation}

```



### TODO: Potential Limitations of Models
```{python}

```




## TODO: Conclusion & Next Steps
```{python }

```