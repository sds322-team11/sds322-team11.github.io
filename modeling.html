<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Modeling</title>

<script src="site_libs/header-attrs-2.17/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Housing Price Prediction</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Project Info</a>
</li>
<li>
  <a href="analysis.html">Analysis &amp; Hypotheses</a>
</li>
<li>
  <a href="modeling.html">Modeling</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Modeling</h1>

</div>


<div id="modeling-summary" class="section level2">
<h2>Modeling Summary</h2>
<p>In order to complete the model training step, we will follow the
following steps from class:</p>
<ol style="list-style-type: decimal">
<li><p>Define your Model Class</p></li>
<li><p>Define the Cost Function</p></li>
<li><p>Perform Optimization</p></li>
<li><p>Check the Performance of Fitted Model</p></li>
</ol>
</div>
<div id="defining-our-model-class" class="section level2">
<h2>Defining Our Model class</h2>
<p>We know that our output will be a number so we will use some type of
regression model. Let’s try out a few regression models</p>
</div>
<div id="import-model-libraries" class="section level2">
<h2>Import model libraries</h2>
<pre class="python"><code>import sklearn
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
matplotlib.use(&#39;Agg&#39;)</code></pre>
</div>
<div id="data-preparation" class="section level2">
<h2>Data Preparation</h2>
<div id="pre-processing-1-normalizing-data" class="section level3">
<h3>Pre-processing 1: Normalizing data</h3>
<pre class="python"><code>df = pd.read_csv(&#39;clean_train.csv&#39;)

# create a scaler object
std_scaler = StandardScaler()
# fit and transform the data
numeric_cols = list(df.select_dtypes(include=[&#39;int64&#39;,&#39;float64&#39;]).columns)

# Remove SalePrice from the values to be normalized
numeric_cols.remove(&quot;SalePrice&quot;)
print(df[numeric_cols])
# Normalize all numeric values</code></pre>
<pre><code>##         Id  LotFrontage  LotArea  LotShape  ...  PoolQC  Fence  MiscVal  YrSold
## 0        1         65.0     8450         0  ...       0      0        0    2008
## 1        2         80.0     9600         0  ...       0      0        0    2007
## 2        3         68.0    11250         1  ...       0      0        0    2008
## 3        4         60.0     9550         1  ...       0      0        0    2006
## 4        5         84.0    14260         1  ...       0      0        0    2008
## ...    ...          ...      ...       ...  ...     ...    ...      ...     ...
## 1451  1456         62.0     7917         0  ...       0      0        0    2007
## 1452  1457         85.0    13175         0  ...       0      3        0    2010
## 1453  1458         66.0     9042         0  ...       0      4     2500    2010
## 1454  1459         68.0     9717         0  ...       0      0        0    2010
## 1455  1460         75.0     9937         0  ...       0      0        0    2008
## 
## [1456 rows x 57 columns]</code></pre>
<pre class="python"><code>df[numeric_cols] = pd.DataFrame(std_scaler.fit_transform(df[numeric_cols]),
  columns=numeric_cols)
print(df[numeric_cols])</code></pre>
<pre><code>##             Id  LotFrontage   LotArea  ...     Fence   MiscVal    YrSold
## 0    -1.729139    -0.203664 -0.202770  ... -0.469568 -0.088475  0.137472
## 1    -1.726767     0.447241 -0.086107  ... -0.469568 -0.088475 -0.615009
## 2    -1.724395    -0.073483  0.081281  ... -0.469568 -0.088475  0.137472
## 3    -1.722023    -0.420632 -0.091179  ... -0.469568 -0.088475 -1.367490
## 4    -1.719651     0.620816  0.386636  ... -0.469568 -0.088475  0.137472
## ...        ...          ...       ...  ...       ...       ...       ...
## 1451  1.722179    -0.333845 -0.256842  ... -0.469568 -0.088475 -0.615009
## 1452  1.724551     0.664209  0.276566  ...  2.022622 -0.088475  1.642435
## 1453  1.726923    -0.160270 -0.142714  ...  2.853352  4.944023  1.642435
## 1454  1.729295    -0.073483 -0.074237  ... -0.469568 -0.088475  1.642435
## 1455  1.731667     0.230273 -0.051919  ... -0.469568 -0.088475  0.137472
## 
## [1456 rows x 57 columns]</code></pre>
</div>
<div
id="pre-processing-2-creating-dummy-variables-for-categorircal-variables"
class="section level3">
<h3>Pre-processing 2: Creating Dummy Variables for categorircal
variables</h3>
<pre class="python"><code>

# Create dummy variable columns for the categorical variables
categorical_columns = df.select_dtypes(include=[&#39;object&#39;,&#39;bool&#39;]).columns
print(df[&#39;GarageType&#39;])</code></pre>
<pre><code>## 0       Attchd
## 1       Attchd
## 2       Attchd
## 3       Detchd
## 4       Attchd
##          ...  
## 1451    Attchd
## 1452    Attchd
## 1453    Attchd
## 1454    Attchd
## 1455    Attchd
## Name: GarageType, Length: 1456, dtype: object</code></pre>
<pre class="python"><code>for column in categorical_columns:
    # Print out the column names
    print(f&quot;column {column}: &quot;,df[column].unique())
    dummies = pd.get_dummies(df[column]).rename(columns= lambda x: column +&#39;_&#39; + str(x))
    df = pd.concat([df, dummies], axis=1)
    df = df.drop([column], axis=1)</code></pre>
<pre><code>## column MSSubClass:  [&#39;2-STORY 1946+&#39; &#39;1-STORY 1946+&#39; &#39;2-STORY 1945-&#39; &#39;1-1/2 STORY FIN&#39;
##  &#39;2 FAMILY CONVERSION&#39; &#39;1-1/2 STORY UNF&#39; &#39;DUPLEX&#39; &#39;1-STORY PUD 1946+&#39;
##  &#39;1-STORY 1945-&#39; &#39;SPLIT FOYER&#39; &#39;SPLIT OR MULTI-LEVEL&#39; &#39;2-STORY PUD 1946+&#39;
##  &#39;2-1/2 STORY&#39; &#39;MULTILEVEL PUD&#39; &#39;1-STORY W/ ATTIC&#39;]
## column MSZoning:  [&#39;RL&#39; &#39;RM&#39; &#39;C&#39; &#39;FV&#39; &#39;RH&#39;]
## column Street:  [&#39;Pave&#39; &#39;Grvl&#39;]
## column Alley:  [nan &#39;Grvl&#39; &#39;Pave&#39;]
## column LandContour:  [&#39;Lvl&#39; &#39;Bnk&#39; &#39;Low&#39; &#39;HLS&#39;]
## column LotConfig:  [&#39;Inside&#39; &#39;FR2&#39; &#39;Corner&#39; &#39;CulDSac&#39; &#39;FR3&#39;]
## column Neighborhood:  [&#39;CollgCr&#39; &#39;Veenker&#39; &#39;Crawfor&#39; &#39;NoRidge&#39; &#39;Mitchel&#39; &#39;Somerst&#39; &#39;NWAmes&#39;
##  &#39;OldTown&#39; &#39;BrkSide&#39; &#39;Sawyer&#39; &#39;NridgHt&#39; &#39;NAmes&#39; &#39;SawyerW&#39; &#39;IDOTRR&#39;
##  &#39;MeadowV&#39; &#39;Edwards&#39; &#39;Timber&#39; &#39;Gilbert&#39; &#39;StoneBr&#39; &#39;ClearCr&#39; &#39;NPkVill&#39;
##  &#39;Blmngtn&#39; &#39;BrDale&#39; &#39;SWISU&#39; &#39;Blueste&#39;]
## column Condition1:  [&#39;Norm&#39; &#39;Feedr&#39; &#39;PosN&#39; &#39;Artery&#39; &#39;RRAe&#39; &#39;RRNn&#39; &#39;RRAn&#39; &#39;PosA&#39; &#39;RRNe&#39;]
## column Condition2:  [&#39;Norm&#39; &#39;Artery&#39; &#39;RRNn&#39; &#39;Feedr&#39; &#39;PosA&#39; &#39;PosN&#39; &#39;RRAn&#39; &#39;RRAe&#39;]
## column BldgType:  [&#39;1Fam&#39; &#39;2fmCon&#39; &#39;Duplex&#39; &#39;TwnhsE&#39; &#39;Twnhs&#39;]
## column HouseStyle:  [&#39;2Story&#39; &#39;1Story&#39; &#39;1.5Fin&#39; &#39;1.5Unf&#39; &#39;SFoyer&#39; &#39;SLvl&#39; &#39;2.5Unf&#39; &#39;2.5Fin&#39;]
## column RoofStyle:  [&#39;Gable&#39; &#39;Hip&#39; &#39;Gambrel&#39; &#39;Mansard&#39; &#39;Flat&#39; &#39;Shed&#39;]
## column RoofMatl:  [&#39;CompShg&#39; &#39;WdShngl&#39; &#39;Metal&#39; &#39;WdShake&#39; &#39;Membran&#39; &#39;Tar&amp;Grv&#39; &#39;Roll&#39;]
## column Exterior1st:  [&#39;VinylSd&#39; &#39;MetalSd&#39; &#39;Wd Sdng&#39; &#39;HdBoard&#39; &#39;BrkFace&#39; &#39;WdShing&#39; &#39;CemntBd&#39;
##  &#39;Plywood&#39; &#39;AsbShng&#39; &#39;Stucco&#39; &#39;BrkComm&#39; &#39;AsphShn&#39; &#39;Stone&#39; &#39;ImStucc&#39;
##  &#39;CBlock&#39;]
## column Exterior2nd:  [&#39;VinylSd&#39; &#39;MetalSd&#39; &#39;WdShing&#39; &#39;HdBoard&#39; &#39;Plywood&#39; &#39;Wd Sdng&#39; &#39;CmentBd&#39;
##  &#39;BrkFace&#39; &#39;Stucco&#39; &#39;AsbShng&#39; &#39;BrkComm&#39; &#39;ImStucc&#39; &#39;AsphShn&#39; &#39;Stone&#39;
##  &#39;Other&#39; &#39;CBlock&#39;]
## column MasVnrType:  [&#39;BrkFace&#39; &#39;None&#39; &#39;Stone&#39; &#39;BrkCmn&#39;]
## column Foundation:  [&#39;PConc&#39; &#39;CBlock&#39; &#39;BrkTil&#39; &#39;Wood&#39; &#39;Slab&#39; &#39;Stone&#39;]
## column Heating:  [&#39;GasA&#39; &#39;GasW&#39; &#39;Grav&#39; &#39;Wall&#39; &#39;OthW&#39; &#39;Floor&#39;]
## column GarageType:  [&#39;Attchd&#39; &#39;Detchd&#39; &#39;BuiltIn&#39; &#39;CarPort&#39; nan &#39;Basment&#39; &#39;2Types&#39;]
## column MiscFeature:  [nan &#39;Shed&#39; &#39;Gar2&#39; &#39;Othr&#39; &#39;TenC&#39;]
## column MoSold:  [&#39;Feb&#39; &#39;May&#39; &#39;Sept&#39; &#39;Dec&#39; &#39;Oct&#39; &#39;Aug&#39; &#39;Nov&#39; &#39;Apr&#39; &#39;Jan&#39; &#39;July&#39; &#39;Mar&#39;
##  &#39;June&#39;]
## column SaleType:  [&#39;WD&#39; &#39;New&#39; &#39;COD&#39; &#39;ConLD&#39; &#39;ConLI&#39; &#39;CWD&#39; &#39;ConLw&#39; &#39;Con&#39; &#39;Oth&#39;]
## column SaleCondition:  [&#39;Normal&#39; &#39;Abnorml&#39; &#39;Partial&#39; &#39;AdjLand&#39; &#39;Alloca&#39; &#39;Family&#39;]</code></pre>
<pre class="python"><code>print(df.columns)</code></pre>
<pre><code>## Index([&#39;Id&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;LotShape&#39;, &#39;Utilities&#39;, &#39;LandSlope&#39;,
##        &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;,
##        ...
##        &#39;SaleType_ConLw&#39;, &#39;SaleType_New&#39;, &#39;SaleType_Oth&#39;, &#39;SaleType_WD&#39;,
##        &#39;SaleCondition_Abnorml&#39;, &#39;SaleCondition_AdjLand&#39;,
##        &#39;SaleCondition_Alloca&#39;, &#39;SaleCondition_Family&#39;, &#39;SaleCondition_Normal&#39;,
##        &#39;SaleCondition_Partial&#39;],
##       dtype=&#39;object&#39;, length=243)</code></pre>
<pre class="python"><code>print(df[[&#39;GarageType_Attchd&#39;, &#39;GarageType_Detchd&#39;]])
</code></pre>
<pre><code>##       GarageType_Attchd  GarageType_Detchd
## 0                     1                  0
## 1                     1                  0
## 2                     1                  0
## 3                     0                  1
## 4                     1                  0
## ...                 ...                ...
## 1451                  1                  0
## 1452                  1                  0
## 1453                  1                  0
## 1454                  1                  0
## 1455                  1                  0
## 
## [1456 rows x 2 columns]</code></pre>
</div>
<div id="pre-processing-3-dropping-na-rows" class="section level3">
<h3>Pre-processing 3: Dropping NA rows</h3>
<pre class="python"><code>rows1 = df.shape[0]
df.dropna(inplace=True)
rows2 = df.shape[0]

print(f&quot;&quot;&quot;
Rows before: {rows1}
Rows after: {rows2}
-----------------------------
Rows dropped: {rows1 - rows2}
&quot;&quot;&quot;)</code></pre>
<pre><code>## 
## Rows before: 1456
## Rows after: 1197
## -----------------------------
## Rows dropped: 259</code></pre>
</div>
<div id="pre-processing-4-creating-test-and-training-datasets"
class="section level3">
<h3>Pre-processing 4: Creating test and training datasets</h3>
<pre class="python"><code>
# Create dataframes for independent and dependent variables
# Dependent variable
y = df[&#39;SalePrice&#39;]
# Independent Variable
X = df.drop(&#39;SalePrice&#39;, axis=1)

# Create training and test sets. Test is .25 of data
(X_train, X_test, y_train, y_test) = train_test_split(X, y,  test_size=.25, random_state=123)</code></pre>
</div>
</div>
<div id="random-forest-regressor" class="section level2">
<h2>Random Forest Regressor</h2>
<pre class="python"><code>from sklearn.ensemble import RandomForestRegressor
rf_mses = []
rf_training_mses = []
max_depths = []
for d in range(1,30):
    rf_regr = RandomForestRegressor(max_depth=d, random_state=0)
    # Fit the data to the model
    rf_regr.fit(X_train, y_train)
    # Make predictions on test data
    rf_y_pred = rf_regr.predict(X_test)
    rf_y_training_pred = rf_regr.predict(X_train)
    # Report Mean Square Error
    rf_mses.append(mean_squared_error(y_test, rf_y_pred))
    rf_training_mses.append(mean_squared_error(y_train, rf_y_training_pred))
    max_depths.append(d)
    #print(f&quot;Mean Squared Error: {rf_mses[-1]}&quot;)</code></pre>
<pre><code>## RandomForestRegressor(max_depth=1, random_state=0)
## RandomForestRegressor(max_depth=2, random_state=0)
## RandomForestRegressor(max_depth=3, random_state=0)
## RandomForestRegressor(max_depth=4, random_state=0)
## RandomForestRegressor(max_depth=5, random_state=0)
## RandomForestRegressor(max_depth=6, random_state=0)
## RandomForestRegressor(max_depth=7, random_state=0)
## RandomForestRegressor(max_depth=8, random_state=0)
## RandomForestRegressor(max_depth=9, random_state=0)
## RandomForestRegressor(max_depth=10, random_state=0)
## RandomForestRegressor(max_depth=11, random_state=0)
## RandomForestRegressor(max_depth=12, random_state=0)
## RandomForestRegressor(max_depth=13, random_state=0)
## RandomForestRegressor(max_depth=14, random_state=0)
## RandomForestRegressor(max_depth=15, random_state=0)
## RandomForestRegressor(max_depth=16, random_state=0)
## RandomForestRegressor(max_depth=17, random_state=0)
## RandomForestRegressor(max_depth=18, random_state=0)
## RandomForestRegressor(max_depth=19, random_state=0)
## RandomForestRegressor(max_depth=20, random_state=0)
## RandomForestRegressor(max_depth=21, random_state=0)
## RandomForestRegressor(max_depth=22, random_state=0)
## RandomForestRegressor(max_depth=23, random_state=0)
## RandomForestRegressor(max_depth=24, random_state=0)
## RandomForestRegressor(max_depth=25, random_state=0)
## RandomForestRegressor(max_depth=26, random_state=0)
## RandomForestRegressor(max_depth=27, random_state=0)
## RandomForestRegressor(max_depth=28, random_state=0)
## RandomForestRegressor(max_depth=29, random_state=0)</code></pre>
<pre class="python"><code>best_n = max_depths[rf_mses.index(min(rf_mses))]
rf_mse = min(rf_mses)
print(f&quot;Best MSE of {rf_mse} at max_depth of {best_n}&quot;)</code></pre>
<pre><code>## Best MSE of 627261421.4452841 at max_depth of 20</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
plt.close()
plt.plot(max_depths, rf_mses)
plt.plot(max_depths, rf_training_mses)
plt.title(&#39;MSE of different max_depth for Random Forest Regressor&#39;)
plt.xlabel(&#39;Max Depth&#39;, size=20)
plt.ylabel(&#39;Mean Square Error&#39;, size=20)
plt.legend([&#39;Testing Data MSE&#39;,&quot;Training Data MSE&quot;])
plt.show()

# Now that we know the best n, train again</code></pre>
<p><img src="modeling_files/figure-html/RFRegressor-1.png" width="672" /></p>
<pre class="python"><code>rf_regr = RandomForestRegressor(max_depth=best_n, random_state=0)
# Fit the data to the model
rf_regr.fit(X_train, y_train)
# Make predictions on test data</code></pre>
<pre><code>## RandomForestRegressor(max_depth=20, random_state=0)</code></pre>
<pre class="python"><code>rf_y_pred = rf_regr.predict(X_test)</code></pre>
<div id="defining-your-model-class" class="section level3">
<h3>Defining your Model Class</h3>
<p>Our model class is scikit-learn’s random forest regressor. The model
class works by building decision trees on different sub-samples of the
data and uses averaging to improve the predictive accuracy. More
information abou the model can be found
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank">
Here</a></p>
</div>
<div id="defining-the-cost-function" class="section level3">
<h3>Defining the Cost Function</h3>
<p>The cost function used for our random forest regressor is mean
squared error. We chose this because it only slightly penalizes the
model for being slightly off, but greatly penalizes the model when it is
off a greater amount.</p>
</div>
<div id="perform-optimization" class="section level3">
<h3>Perform Optimization</h3>
<p>We optimized the performance of our model by trying different
max-depths for the random forest model. This helped us avoid over
fitting. We went with a model that would minimize the mean square error
of the model</p>
</div>
<div id="check-rf-performance" class="section level3">
<h3>Check RF Performance</h3>
<p>The performance of the model is checked below:</p>
<pre class="python"><code>rf_mse = mean_squared_error(y_test, rf_y_pred)
print(f&quot;Mean Squared Error: {rf_mse}&quot;)</code></pre>
<pre><code>## Mean Squared Error: 627261421.4452841</code></pre>
<pre class="python"><code>max_val = max(max(y_test),max(rf_y_pred))
plt.close()
plt.plot([0.0, max_val], [0.0, max_val], &#39;k&#39;)
plt.plot(rf_y_pred, y_test, &#39;.y&#39;)
plt.xlabel(&#39;Random Forest Regressor Predicted Sales Price ($)&#39;, size=20)
plt.ylabel(&#39;True Sales Price ($)&#39;, size=20)
plt.title(f&#39;Random Forest Regressor Diagnosis Plot (max_depth={best_n})&#39;)
plt.show()
# Report Mean Square Error</code></pre>
<p><img src="modeling_files/figure-html/RFRegressor-Eval-3.png" width="672" /></p>
<pre class="python"><code>print(f&quot;Random Forest RMSE: {rf_mse:,}&quot;)</code></pre>
<pre><code>## Random Forest RMSE: 627,261,421.4452841</code></pre>
<pre class="python"><code>print(f&quot;Random Forest R^2: {rf_regr.score(X_test, y_test)}&quot;)</code></pre>
<pre><code>## Random Forest R^2: 0.9053810176351551</code></pre>
</div>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<pre class="python"><code># Train a linear model
lin_regr = LinearRegression(fit_intercept=False)
lin_regr.fit(X_train, y_train)
# Make predictions on test data</code></pre>
<pre><code>## LinearRegression(fit_intercept=False)</code></pre>
<pre class="python"><code>lin_y_pred = lin_regr.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_y_pred)</code></pre>
<pre class="python"><code>import statsmodels.api as sm
from scipy import stats

X2 = sm.add_constant(X_train)
est = sm.OLS(y_train, X2)
lin_regr = est.fit()
#print(lin_regr.summary())
lin_y_pred = lin_regr.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_y_pred)
print(lin_mse)</code></pre>
<pre><code>## 915059405.4317662</code></pre>
<pre class="python"><code>print(lin_regr.rsquared)</code></pre>
<pre><code>## 0.9361021808498309</code></pre>
<pre class="python"><code>max_val = max(max(y_test),max(lin_y_pred))
plt.close()
plt.plot([0.0, max_val], [0.0, max_val], &#39;k&#39;)
plt.plot(lin_y_pred, y_test, &#39;.r&#39;)
plt.xlabel(&#39;Linear Model Predicted Sales Price ($)&#39;, size=20)
plt.ylabel(&#39;True Sales Price ($)&#39;, size=20)
plt.title(&#39;Linear Model Diagnosis Plot&#39;)
plt.legend([&#39;Perfect fit line&#39;,&quot;Linear Model&quot;])
plt.show()</code></pre>
<p><img src="modeling_files/figure-html/Linear-Eval-5.png" width="672" /></p>
<pre class="python"><code>print(f&quot;Linear MSE: {lin_mse:,}&quot;)</code></pre>
<pre><code>## Linear MSE: 915,059,405.4317662</code></pre>
<pre class="python"><code>print(f&quot;Linear Reg R^2: {lin_regr.rsquared}&quot;)</code></pre>
<pre><code>## Linear Reg R^2: 0.9361021808498309</code></pre>
</div>
<div id="knn-regressor" class="section level2">
<h2>KNN Regressor</h2>
<pre class="python"><code>from sklearn.neighbors import KNeighborsRegressor
knn_mses = []
nums_neighbors = []
for k in range(1,30):
    knn_regr = KNeighborsRegressor(
      n_neighbors=k,
      weights=&quot;distance&quot;
    )
    # Fit the data to the model
    knn_regr.fit(X_train, y_train)
    # Make predictions on test data
    knn_y_pred = knn_regr.predict(X_test)
    # Report Mean Square Error
    knn_mses.append(mean_squared_error(y_test, knn_y_pred))
    nums_neighbors.append(k)
    #print(f&quot;Mean Squared Error: {knn_mses[-1]}&quot;)</code></pre>
<pre><code>## KNeighborsRegressor(n_neighbors=1, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=2, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=3, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=4, weights=&#39;distance&#39;)
## KNeighborsRegressor(weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=6, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=7, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=8, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=9, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=10, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=11, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=12, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=13, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=14, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=15, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=16, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=17, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=18, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=19, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=20, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=21, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=22, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=23, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=24, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=25, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=26, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=27, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=28, weights=&#39;distance&#39;)
## KNeighborsRegressor(n_neighbors=29, weights=&#39;distance&#39;)</code></pre>
<pre class="python"><code>best_k = nums_neighbors[knn_mses.index(min(knn_mses))]
knn_mse = min(knn_mses)
print(f&quot;Best MSE of {knn_mse} at n_neighbors of {best_k}&quot;)</code></pre>
<pre><code>## Best MSE of 776224614.0781039 at n_neighbors of 5</code></pre>
<pre class="python"><code>plt.close()
plt.plot(nums_neighbors, knn_mses, c=&quot;red&quot;)
plt.title(&#39;MSE of different n_neighbors for KNN&#39;)
plt.xlabel(&#39;Number of Neighbors&#39;, size=20)
plt.ylabel(&#39;Mean Square Error&#39;, size=20)

plt.rcParams[&quot;figure.autolayout&quot;] = True
plt.show()



# Now that we know the best n, train again</code></pre>
<p><img src="modeling_files/figure-html/KNNRegressor-7.png" width="672" /></p>
<pre class="python"><code>knn_regr = KNeighborsRegressor(
  n_neighbors=best_k,
  weights=&quot;distance&quot;
  )
# Fit the data to the model
knn_regr.fit(X_train, y_train)
# Make predictions on test data</code></pre>
<pre><code>## KNeighborsRegressor(weights=&#39;distance&#39;)</code></pre>
<pre class="python"><code>knn_y_pred = knn_regr.predict(X_test)
</code></pre>
<div id="defining-your-model-class-1" class="section level3">
<h3>Defining your Model Class</h3>
<p>Our model class is scikit-learn’s K-nearest neighbors regressor. The
model class works by basically memorizing a list of points for comparing
future points to - these future points are compared to the k nearest
neighbors and take the weighted average of their values (weights are
determined by distance). More information abou the model can be found
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank">
Here</a></p>
</div>
<div id="defining-the-cost-function-1" class="section level3">
<h3>Defining the Cost Function</h3>
<p>KNN does not exactly have a cost function - instead, the KNN model
basically just memorizes the locations of the training dataset and
compares future prediction points to the points the model was trained
on. The model then chooses the weighted average of the k-closest
points</p>
</div>
<div id="perform-optimization-1" class="section level3">
<h3>Perform Optimization</h3>
<p>We optimized the performance of our model by trying different numbers
of neighbors (k) for the KNN model. This helped us determine which K
values would be best for our model.</p>
</div>
<div id="check-knn-performance" class="section level3">
<h3>Check KNN Performance</h3>
<p>The performance of the model is checked below:</p>
<pre class="python"><code>plt.close()
max_val = max(max(y_test),max(knn_y_pred))
plt.plot([0.0, max_val], [0.0, max_val], &#39;k&#39;)
plt.plot(knn_y_pred, y_test, &#39;.b&#39;)
plt.xlabel(&#39;KNN Model Predicted Sales Price ($)&#39;, size=20)
plt.ylabel(&#39;True Sales Price ($)&#39;, size=20)
plt.title(f&#39;KNN Regressor Diagnosis Plot (neighbors={best_k})&#39;)
plt.legend([&#39;Perfect fit line&#39;,&quot;KNN Model&quot;])
plt.show()

# Report Mean Square Error</code></pre>
<p><img src="modeling_files/figure-html/KNN-eval-9.png" width="672" /></p>
<pre class="python"><code>print(f&quot;MSE of KNN: {mean_squared_error(y_test, knn_y_pred)}&quot;)</code></pre>
<pre><code>## MSE of KNN: 776224614.0781039</code></pre>
<pre class="python"><code>print(f&quot;KNN R^2 score: {knn_regr.score(X_test, y_test)}&quot;)</code></pre>
<pre><code>## KNN R^2 score: 0.8829107281914653</code></pre>
</div>
</div>
<div id="neural-network-regressor" class="section level2">
<h2>Neural Network Regressor</h2>
<pre class="python"><code>from sklearn.neural_network import MLPRegressor
vals = []
combos = []
mlp_regr = MLPRegressor(random_state=1,
  max_iter=500, 
  hidden_layer_sizes=(100,30,8),
  learning_rate=&quot;constant&quot;, # {‘constant’, ‘invscaling’, ‘adaptive’}
  alpha=10**-6, # default =0.0001
  solver=&quot;adam&quot;,
  batch_size=200
  ).fit(X_train, y_train)</code></pre>
<pre><code>## /stor/home/dcd2287/.local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn&#39;t converged yet.
##   % self.max_iter, ConvergenceWarning)</code></pre>
<pre class="python"><code>MLP_y_pred = mlp_regr.predict(X_test)
print(mlp_regr.score(X_test, y_test))</code></pre>
<pre><code>## 0.9190756873516708</code></pre>
<div id="defining-your-model-class-2" class="section level3">
<h3>Defining your Model Class</h3>
<p>Our model class is scikit-learn’s MLP. The model works by creating
and training a neural network in epochs. We trained parameters More
information about the model can be found
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html" target="_blank">
Here</a></p>
</div>
<div id="defining-the-cost-function-2" class="section level3">
<h3>Defining the Cost Function</h3>
<p>The cost function in the neural network minimizes the squared error
using the “adam” stochastic gradient-based optimizer</p>
</div>
<div id="perform-optimization-2" class="section level3">
<h3>Perform Optimization</h3>
<p>We tried to optimize the performance of the neural network by trying
different hyperparameters for the model and keeping the new
hyperparameters if the squared error improved.</p>
</div>
<div id="check-neural-net-performance" class="section level3">
<h3>Check Neural Net Performance</h3>
<p>The performance of the model is checked below:</p>
<pre class="python"><code>plt.close()
max_val = max(max(y_test),max(MLP_y_pred))
plt.plot([0.0, max_val], [0.0, max_val], &#39;k&#39;)
plt.plot(MLP_y_pred, y_test, &#39;.g&#39;)
plt.xlabel(&#39;Neural Network Model Predicted Sales Price ($)&#39;, size=20)
plt.ylabel(&#39;True Sales Price (%)&#39;, size=20)
plt.title(&#39;Neural Network Model Diagnosis Plot&#39;)
plt.legend([&#39;Perfect fit line&#39;,&quot;Neural Network Model&quot;])
plt.show()

# Report Mean Square Error</code></pre>
<p><img src="modeling_files/figure-html/NeuralNetwork-eval-11.png" width="672" /></p>
<pre class="python"><code>print(f&quot;MSE of Neural Network: {mean_squared_error(y_test, MLP_y_pred)}&quot;)</code></pre>
<pre><code>## MSE of Neural Network: 536474797.26154166</code></pre>
<pre class="python"><code>print(f&quot;Neural Network R^2 score: {mlp_regr.score(X_test, y_test)}&quot;)</code></pre>
<pre><code>## Neural Network R^2 score: 0.9190756873516708</code></pre>
</div>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<div id="measuring-performance" class="section level3">
<h3>Measuring Performance</h3>
<pre class="python"><code>
lin_mse = mean_squared_error(y_test, lin_y_pred)
knn_mse = mean_squared_error(y_test, knn_y_pred)
rf_mse = mean_squared_error(y_test, rf_y_pred)
MLP_mse = mean_squared_error(y_test, MLP_y_pred)

max_val = max(
  max(y_test),
  max(MLP_y_pred),
  max(rf_y_pred), 
  max(knn_y_pred)
)
plt.close()
plt.xlim([0, max_val])</code></pre>
<pre><code>## (0.0, 582933.0)</code></pre>
<pre class="python"><code>plt.xlim([0, max_val])</code></pre>
<pre><code>## (0.0, 582933.0)</code></pre>
<pre class="python"><code>plt.plot([0.0, max_val], [0.0, max_val], &#39;k&#39;)
plt.plot(lin_y_pred, y_test, &#39;.r&#39;,alpha=0.3)
plt.plot(knn_y_pred, y_test, &#39;.b&#39;,alpha=0.3)
plt.plot(rf_y_pred, y_test, &#39;.y&#39;,alpha=0.3)
plt.plot(MLP_y_pred, y_test, &#39;.g&#39;,alpha=0.3)
# apply legend()
plt.xlabel(&#39;Model Predicted Sales Price ($)&#39;, size=20)
plt.ylabel(&#39;True Sales Price ($)&#39;, size=20)
plt.title(&#39;Model Diagnosis Plot (all models)&#39;)

plt.legend([&#39;Perfect fit line&#39;,&quot;Linear Model&quot;,f&quot;KNN Model k={best_k}&quot;,f&quot;RF Model md={best_n}&quot;, &quot;Neural Network Model&quot;])
plt.show()</code></pre>
<p><img src="modeling_files/figure-html/performance-eval-13.png" width="672" /></p>
<pre class="python"><code>print(f&quot;&quot;&quot;
MSEs (Mean Square Errors)---------------
Linear Reg:        {lin_mse:,}
KNN Reg:           {knn_mse:,}
Random Forest Reg: {rf_mse:,}
Neural Network Reg: {MLP_mse:,}
R^2 Scores---------------
Linear Reg: {lin_regr.rsquared}
KNN Reg: {knn_regr.score(X_test, y_test)}
Random Forest Reg: {rf_regr.score(X_test, y_test)}
Neural Network Reg: {mlp_regr.score(X_test, y_test)}&quot;&quot;&quot;)</code></pre>
<pre><code>## 
## MSEs (Mean Square Errors)---------------
## Linear Reg:        915,059,405.4317662
## KNN Reg:           776,224,614.0781039
## Random Forest Reg: 627,261,421.4452841
## Neural Network Reg: 536,474,797.26154166
## R^2 Scores---------------
## Linear Reg: 0.9361021808498309
## KNN Reg: 0.8829107281914653
## Random Forest Reg: 0.9053810176351551
## Neural Network Reg: 0.9190756873516708</code></pre>
</div>
<div id="todo-findings-and-interpretations" class="section level3">
<h3>TODO: Findings and Interpretations</h3>
<p>Based on the R^2 and MSE values of the models when test data is
applied to them,</p>
</div>
<div id="todo-potential-limitations-of-models" class="section level3">
<h3>TODO: Potential Limitations of Models</h3>
</div>
</div>
<div id="todo-conclusion-next-steps" class="section level2">
<h2>TODO: Conclusion &amp; Next Steps</h2>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
